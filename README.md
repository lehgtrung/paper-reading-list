# papers-courses-books-talks

## Books
1. Statistical Inference [PDF](https://mybiostats.files.wordpress.com/2015/03/casella-berger.pdf)
2. Deep Learning on Graph [Link](https://cse.msu.edu/~mayao4/dlg_book/)
3. Electric Power System Basics For Non-electrical Professional 2nd Edition, Steve W. Blume

## Talks
1. Beyond NP with Tractable Circuits [Youtube](https://www.youtube.com/watch?v=kdMzmgyLfQs&t=2357s)
2. Bridging Machine Learning and Logical Reasoning by Abductive Learning [Youtube](https://www.youtube.com/watch?v=ETHrFxiFIUM&t=2752s)

## Courses
1. **CS264A Automated Reasoning., UCLA** 
    * [Youtube playlist](https://www.youtube.com/playlist?list=PLlDG_zCuBub5AyHuxnw8vfgx7Wd-P-4XN)
    * [Review Note](http://web.cs.ucla.edu/~patricia.xiao/files/CS264A_Review_Note_midterm.pdf)
    * [Homeworks](https://github.com/lehgtrung/UCLA-CS264A-Fall2020)

2. **CS224W: Machine Learning with Graphs., Stanford** 
    * [Youtube playlist](https://www.youtube.com/watch?v=JAB_plj2rbA&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn)
    * [Website](http://web.stanford.edu/class/cs224w/)

3. **Graph Neural Network., UPenn**
    * [Course page](https://gnn.seas.upenn.edu/lectures/)

## Papers

### Abductive learning
1. **Abductive Knowledge Induction From Raw Data** IJCAI 2020, [paper](https://arxiv.org/pdf/2010.03514.pdf)
    
    *Wang-Zhou Dai , Stephen H. Muggleton*
    
    Summary: TODO
    
1. **Abductive learning: towards bridging machine learning and logical reasoning** 2019, [paper](http://scis.scichina.com/en/2019/076101.pdf)
    
    *Zhi-Hua ZHOU*
    
    Summary: TODO
    
1. **Semi-Supervised Abductive Learning and Its Application to Theft Judicial Sentencing** ICDM 2020, [paper](https://cs.nju.edu.cn/liyf/paper/icdm20-SSABL.pdf)

    *Yu-Xuan Huang et al*
    
    Summary: TODO

### Neural-sympolic Learning
1. **A Semantic Loss Function for Deep Learning with Symbolic Knowledge** ICLM'2020, [paper](https://arxiv.org/abs/1711.11157)

    *Jingyi Xu 1 Zilu Zhang 2 Tal Friedman 1 Yitao Liang 1 Guy Van den Broeck*
    
    Summary: TODO

1. **Embedding Symbolic Knowledge into Deep Networks** NeurIPS'2019, [paper](https://arxiv.org/abs/1909.01161)

    *Yaqi Xie, Ziwei Xu, Mohan S Kankanhalli, Kuldeep S. Meel, Harold Soh* 
    
    **Summary**: [Chris's Blog](http://christopher5106.github.io/deep/learning/2020/02/26/symbolic_knowledge_in_deep_networks.html)

1. **NeurASP: Embracing Neural Networks into Answer Set Programming** IJCAI-2020, [paper](https://www.ijcai.org/proceedings/2020/0243.pdf)

    *Zhun Yang, Adam Ishay, Joohyung Lee*
    
    Summary: TODO

1. **A Knowledge Compilation Map** Journal of AI Research 2002, [paper](https://arxiv.org/abs/1106.1819)

    *Mehmet Aydar, Ozge Bozal, Furkan Ozbay*

1. **Harnessing Deep Neural Networks with Logic Rules** ACL 2016, [paper](http://www.cs.cmu.edu/~epxing/papers/2016/Hu_etal_ACL16.pdf)

    *Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric P. Xing*


### Neural Relation Extraction (NLP)
1. **Graph Convolution over Pruned Dependency Trees Improves Relation Extraction** ACL, 2018, [paper](https://arxiv.org/pdf/1809.10185.pdf)

    *Yuhao Zhang,* Peng Qi,* Christopher D. Manning*
    
1. **Attention Guided Graph Convolutional Networks for Relation Extraction** ACL, 2019, [paper](https://aclanthology.org/P19-1024.pdf)

    *Zhijiang Guo, Yan Zhang and Wei Lu*
    
1. **Syntax-Aware Opinion Role Labeling with Dependency Graph Convolutional Networks** ACL, 2020, [paper](https://aclanthology.org/2020.acl-main.297.pdf)

    *Bo Zhang, Yue Zhang, Rui Wang, Zhenghua Li, Min Zhang*
    
1. **Learning Latent Forests for Medical Relation Extraction** IJCAI, 2020, [paper](https://www.ijcai.org/Proceedings/2020/0505.pdf)

    *Zhijiang Guo1, Guoshun Nan, Wei Lu and Shay B. Cohen*
    
1. **Leveraging Dependency Forest for Neural Medical Relation Extraction** EMNLP, 2019, [paper](https://arxiv.org/pdf/1911.04123.pdf)

    *Linfeng Song, Yue Zhang, Daniel Gildea, Mo Yu, Zhiguo Wang and Jinsong Su*
    
1. **NERO: A Neural Rule Grounding Framework for Label-Efficient Relation Extraction** WWWW, 2020, [paper](https://arxiv.org/pdf/1909.02177.pdf)

    *Wenxuan Zhou, Hongtao Lin,  Bill Yuchen Lin, Ziqi Wang*
    
### Visual Relation Extraction (CV)
1. **Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation** IJCNN 2019 [paper](https://arxiv.org/pdf/1910.00462v1.pdf)

   *Ivan Donadello, Luciano Serafini*


### Survey and general methods
1. **Neural relation extraction: a survey** arXiv, 2020, [paper](https://arxiv.org/abs/2007.04247)

    *Mehmet Aydar, Ozge Bozal, Furkan Ozbay*

1. **NLP-progress: Relationship Extraction** [Link](http://nlpprogress.com/english/relationship_extraction.html)

1. **Knowledge Graph Embedding: A Survey of Approaches and Applications** TKDE, 2017 [Link](https://persagen.com/files/misc/Wang2017Knowledge.pdf)

    *Quan Wang, Zhendong Mao, Bin Wang, and Li Guo*
1. **A Survey on Knowledge Graphs: Representation, Acquisition, and Applications**, 2021, IEEE Transactions on Neural Networks and Learning Systems, [Link](https://ieeexplore.ieee.org/abstract/document/9416312)
    
    *S. Ji, S. Pan, E. Cambria, P. Marttinen and P. S. Yu*
    
1. **A Roadmap to Domain Knowledge Integration in Machine Learning**, 2020, IEEE International Conference on Knowledge Graph, [Link](https://sci-hub.se/https://ieeexplore.ieee.org/abstract/document/9194557)
    
    *H. D. Gupta and V. S. Sheng*
    
### Answer Set Programming
1. **What Is Answer Set Programming?**, [paper](https://www.cs.utexas.edu/users/vl/papers/wiasp.pdf)

   *Vladimir Lifschitz*
   
1. **Answer Set Programming: Basics**, [paper](https://iccl.inf.tu-dresden.de/w/images/1/1a/FLP-ASP-L1.pdf)

   *Sebastian Rudolph*

### Awesome githubs
1. **Active learning**, https://github.com/yongjin-shin/awesome-active-learning
2. **Few-shot learning**, https://github.com/Bryce1010/Awesome-Few-shot
3. **Transfer learning**, https://github.com/artix41/awesome-transfer-learning
4. **Knowledge distillation**, https://github.com/FLHonker/Awesome-Knowledge-Distillation

## Insights

### Neural-symbollic integration
Learning with rules: The core idea of neural symbollic integration is that we use rules to eliminate hypothesises that are inconsistent with the KB (the same idea as ASP). Therefore, we can maximize the learning capability of machine learning models since the KB has tighten the search space of the model.

The rules we are trying to incoporate should be in form of integrity constrains which means a list conjunct literals that implies FALSE. For example, in Visual Relationship Detection (VDR) the visual relationship <person, ride, horse> is expressed with the atomic formulas Person(p1), Horse(h1) and ride(p1, h1). Common knowledge is expressed through logical constraints, e.g., ∀x, y(ride(x, y) → ¬Dog(x)) states that dogs do not ride.


